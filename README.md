# Flutter FastAPI Chatbot

A modern, real-time chatbot application built with Flutter and FastAPI. This project combines a beautiful Flutter frontend with a powerful FastAPI backend powered by Ollama AI models and the Strands framework for intelligent, context-aware conversations with real-time web data access.

![Flutter](https://img.shields.io/badge/Flutter-3.10.8-02569B?style=flat&logo=flutter)
![FastAPI](https://img.shields.io/badge/FastAPI-0.128.7-009688?style=flat&logo=fastapi)
![Python](https://img.shields.io/badge/Python-3.13+-3776AB?style=flat&logo=python)

## ğŸŒŸ Features

- **Real-time Chat**: WebSocket-based communication for instant messaging
- **AI-Powered Responses**: Utilizes Ollama's Llama 3.2 model for intelligent conversation
- **Web Search Integration**: AI agent can fetch real-time information from the web
- **Modern UI**: Clean, responsive Flutter interface with Material Design
- **Cross-Platform**: Supports Android, iOS, Web, Windows, Linux, and macOS
- **Dark Theme**: Eye-friendly dark mode interface

## ğŸ—ï¸ Architecture

The application follows a client-server architecture:

- **Frontend**: Flutter application with WebSocket client
- **Backend**: FastAPI server with WebSocket support
- **AI Engine**: Ollama running Llama 3.2:3b model
- **Agent Framework**: Strands AI agent with HTTP request tools

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flutter   â”‚ â—„â”€â”€â”€â”€â”€â–º â”‚   FastAPI    â”‚ â—„â”€â”€â”€â”€â”€â–º â”‚   Ollama    â”‚
â”‚   Client    â”‚ WebSocketâ”‚   Server     â”‚          â”‚  (Llama 3.2)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–²
                                â”‚
                                â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Web Search  â”‚
                         â”‚    Tools     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

Before you begin, ensure you have the following installed:

### Backend Requirements
- Python 3.13 or higher
- [Ollama](https://ollama.ai/) installed and running
- uv (Python package manager) - optional but recommended

### Frontend Requirements
- Flutter SDK 3.10.8 or higher
- Dart SDK (included with Flutter)

### Ollama Setup
1. Install Ollama from [ollama.ai](https://ollama.ai/)
2. Pull the Llama 3.2 model:
   ```bash
   ollama pull llama3.2:3b
   ```
3. Ensure Ollama is running on `http://localhost:11434`

## ğŸš€ Installation & Setup

### Backend Setup

1. Navigate to the server directory:
   ```bash
   cd server
   ```

2. Install dependencies using uv (recommended):
   ```bash
   uv sync
   ```
   
   Or using pip:
   ```bash
   pip install -e .
   ```

3. Start the FastAPI server:
   ```bash
   python main.py
   ```
   
   The server will start on `http://localhost:8000`

### Frontend Setup

1. Navigate to the project root directory

2. Install Flutter dependencies:
   ```bash
   flutter pub get
   ```

3. Run the application:
   ```bash
   # For web
   flutter run -d chrome
   
   # For desktop
   flutter run -d windows  # or macos, linux
   
   # For mobile (with device connected)
   flutter run
   ```

## ğŸ’¡ Usage

1. **Start the Backend**: Make sure the FastAPI server is running on port 8000
2. **Start Ollama**: Ensure Ollama is running with the Llama 3.2 model
3. **Launch the App**: Run the Flutter application
4. **Start Chatting**: Type your message in the input field and press send

The AI assistant can:
- Answer general questions using its trained knowledge
- Fetch real-time information from the web when needed
- Provide context-aware responses based on conversation history

## ğŸ“¡ API Endpoints

### WebSocket Endpoint
- **URL**: `ws://localhost:8000/ws/chat`
- **Purpose**: Real-time bidirectional communication
- **Message Format**:
  ```json
  {
    "message": "Your question here"
  }
  ```
- **Response Format**:
  ```json
  {
    "type": "search_result",
    "response": "AI response here"
  }
  ```

### REST Endpoint
- **URL**: `POST http://localhost:8000/chat`
- **Purpose**: Single message/response interaction
- **Request Body**:
  ```json
  {
    "message": "Your question here"
  }
  ```
- **Response**:
  ```json
  {
    "response": "AI response here"
  }
  ```

### Health Check
- **URL**: `GET http://localhost:8000/health`
- **Purpose**: Check server status

## ğŸ“ Project Structure

```
flutter-fastapi-chatbot/
â”œâ”€â”€ lib/                        # Flutter application code
â”‚   â”œâ”€â”€ main.dart              # Application entry point
â”‚   â”œâ”€â”€ pages/                 # Application screens
â”‚   â”‚   â”œâ”€â”€ home_page.dart     # Main home screen
â”‚   â”‚   â””â”€â”€ chat_page.dart     # Chat interface
â”‚   â”œâ”€â”€ services/              # Service layer
â”‚   â”‚   â””â”€â”€ chat_web_services.dart  # WebSocket client
â”‚   â”œâ”€â”€ widgets/               # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ side_bar.dart
â”‚   â”‚   â”œâ”€â”€ search_section.dart
â”‚   â”‚   â””â”€â”€ search_bar_button.dart
â”‚   â””â”€â”€ theme/                 # App theming
â”‚       â””â”€â”€ colors.dart
â”œâ”€â”€ server/                    # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI application
â”‚   â”‚   â”œâ”€â”€ models/           # Data models
â”‚   â”‚   â”‚   â””â”€â”€ chat_model.py
â”‚   â”‚   â”œâ”€â”€ routes/           # API routes
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py       # Chat endpoints
â”‚   â”‚   â”‚   â””â”€â”€ health.py     # Health check
â”‚   â”‚   â””â”€â”€ services/         # Business logic
â”‚   â”‚       â””â”€â”€ chat_services.py  # AI agent integration
â”‚   â”œâ”€â”€ main.py               # Server entry point
â”‚   â””â”€â”€ pyproject.toml        # Python dependencies
â”œâ”€â”€ android/                  # Android platform files
â”œâ”€â”€ ios/                      # iOS platform files
â”œâ”€â”€ web/                      # Web platform files
â”œâ”€â”€ windows/                  # Windows platform files
â”œâ”€â”€ linux/                    # Linux platform files
â”œâ”€â”€ macos/                    # macOS platform files
â””â”€â”€ pubspec.yaml              # Flutter dependencies
```

## ğŸ› ï¸ Technologies Used

### Frontend
- **Flutter**: Cross-platform UI framework
- **Dart**: Programming language
- **google_fonts**: Custom fonts integration
- **web_socket_client**: WebSocket communication

### Backend
- **FastAPI**: Modern Python web framework
- **Uvicorn**: ASGI server
- **Strands**: AI agent framework
- **Strands-tools**: HTTP request tools for web search

### AI/ML
- **Ollama**: Local LLM runtime
- **Llama 3.2**: Language model (3B parameters)

## ğŸ”§ Configuration

### Backend Configuration
The backend can be configured in `server/app/services/chat_services.py`:

- **Ollama Host**: Change the host URL if Ollama is running on a different address
- **Model ID**: Switch to a different Ollama model if desired
- **System Prompt**: Customize the AI assistant's behavior

### Frontend Configuration
The frontend WebSocket URL can be configured in `lib/services/chat_web_services.dart`:

- **WebSocket URL**: Update if the backend is running on a different host/port

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ‘¤ Author

Rudraksh121a

## ğŸ™ Acknowledgments

- [Flutter Team](https://flutter.dev/) for the amazing framework
- [FastAPI](https://fastapi.tiangolo.com/) for the modern Python framework
- [Ollama](https://ollama.ai/) for making LLMs accessible locally
- [Strands](https://github.com/strands-ai/strands) for the AI agent framework
